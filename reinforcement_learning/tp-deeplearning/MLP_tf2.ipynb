{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed427289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc905c",
   "metadata": {},
   "source": [
    "L'objectif de ce TP est de créer un modèle NN simple (MLP), qui utilise des couches \"fully connected\", pour faire la classification de chiffres manuscrites du dataset MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26430d",
   "metadata": {},
   "source": [
    "# Avant de se lancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45396f4",
   "metadata": {},
   "source": [
    "Quelque exercice basique pour comprendre des bases de TF (2.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1db64554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TensorFlow variables et constantes\n",
    "const = tf.constant(2.0, name=\"const\")\n",
    "b = tf.Variable(2.0, name='b')\n",
    "c = tf.Variable(1.0, name='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da897c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b6862d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'b:0' shape=() dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "792a182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create some operations\n",
    "\n",
    "# somme de b et c (tf.add)\n",
    "d = tf.add(b, c, name='d')\n",
    "\n",
    "# substraire const à c (tf.substract)\n",
    "e = tf.subtract(c, const, name='e')\n",
    "\n",
    "# multiplication entre d et e (tf.multiply)\n",
    "a = tf.multiply(d, e, name='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e494a2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable e is: tf.Tensor(-1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Variable e is:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1287cd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable e is: -1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Variable e is:\", e.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ab1603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cela est équivalent aux opérations suivantes\n",
    "d = b + c\n",
    "e = c - 2\n",
    "a = d * e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f44cdc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable e is: -1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Variable e is:\", e.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57aacecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autres opérations utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81c91915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'b:0' shape=(10,) dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>\n"
     ]
    }
   ],
   "source": [
    "b = tf.Variable(np.arange(0, 10), name='b')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "184ba981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliser tf.cast pour transformer b de int à float 32\n",
    "# puis sommer const\n",
    "d = tf.cast(b, tf.float32) + const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "230bdeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "59cc523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'b:0' shape=(10,) dtype=int64, numpy=array([ 0, 10,  2,  3,  4,  5, 10, 10, 10,  9])>\n"
     ]
    }
   ],
   "source": [
    "b[1].assign(10)\n",
    "b[6:9].assign([10, 10, 10])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16298dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 3 4], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "f = b[2:5]\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebd59b",
   "metadata": {},
   "source": [
    "# NN Classification MNIST\n",
    "\n",
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0b097",
   "metadata": {},
   "source": [
    "Importer et préparer le dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6e8bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training dataset.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# # reshape the data by flattening the images\n",
    "# x_train = np.reshape(x_train, (-1, 784))\n",
    "# x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Reserve 10,000 samples from the training data for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d666b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x_data, y_data, batch_size):\n",
    "    idxs = np.random.randint(0, len(y_data), batch_size)\n",
    "    return x_data[idxs,:,:], y_data[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4a131",
   "metadata": {},
   "source": [
    "### Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd235f",
   "metadata": {},
   "source": [
    "Nous voulons construire un modèle MLP (multi layer perceptron).\n",
    "Pour cela, nous avons besoin d'abord de définir les couches de perceptrons qui vont composer le modèle, c'est à dire de couches \"fully connected\" qui prendent en entrée un vecteur de valeurs $x$ et qui donnent en sortie une activation $a = f(W*x + b)$, où:\n",
    "\n",
    "* $W$ est la matrice des poids\n",
    "* $b$ est le vecteur de biais\n",
    "* $f(.)$ est la fonction d'activation\n",
    "\n",
    "Nous voulons créer un réseau MLP à deux couches. la première couche prend en entrée les données des veleurs numériques pour chaque pixel de l'image et est composé par 64 neurones avec activation relu. La deuxième couche prend en entrée les sorties de la première et sort les logits (ou les probabilités) d'appartenence à une classe, c'est à dire la prédiction de la chiffre contenue dans l'image en entrée.\n",
    "\n",
    "Nous allons d'abord définir les poids W et b de la bonne taille pour les deux couches du réseau : ils sont des variables dans tensorflow, car ils changnet lors de l'entrainement.\n",
    "Utiliser tf.random.normal pour initialiser les valeurs des poids de manière aléatoire suivant une distribution normale. Pour les W, utiliser une deviation standard de 0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7bd75256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now declare the weights connecting the input to the hidden layer\n",
    "W1 = tf.Variable(tf.random.normal([784, 300], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random.normal([300]), name='b1')\n",
    "\n",
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random.normal([64, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random.normal([10]), name='b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03147e7d",
   "metadata": {},
   "source": [
    "Maintenant que nous avons à disposition les poids pour construire notre modèle, nous pouvons écrire une fonction qui, à partir d'une ou plusieurs images en entrée calcule la prédiction en sortie.\n",
    "\n",
    "Nous allons construire un modèle qui prend en entrée les images 28x28 et il les aplatie, c'est à dire ils nous donnes des vecteurs de taille 784. Ensuite il est composé de deux couches \"fully connected\", la première avec 64 néurones avec activation ReLU, et une couche finale de classification composée par un nombre de neurones égal au nombre de classes, et sans activation, pour pouvoir donner en sortie la valeur de logit pour chaque classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a50eaa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a NN model with the FCLayer\n",
    "\n",
    "def nn_model(x_input, W1, b1, W2, b2):\n",
    "    \n",
    "    # flatten the input image from 28 x 28 to 784\n",
    "    x_input = tf.reshape(x_input, (x_input.shape[0], -1))\n",
    "    \n",
    "    x = tf.add(tf.matmul(tf.cast(x_input, tf.float32), W1), b1)\n",
    "    \n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    logits = tf.add(tf.matmul(x, W2), b2)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f7146",
   "metadata": {},
   "source": [
    "### Entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda95279",
   "metadata": {},
   "source": [
    "Nous entraînons le modèle en utilisant une méthode déscente de gradients stochastique (avec moni-batches) avec une boucle d'entraînement personnalisée.\n",
    "\n",
    "Tout d'abord, nous avons besoin d'un optimiseur et d'une loss function. Nous choisissons un optimiseur SGD et une loss \"cross entropy\", comme il s'agit d'un problème de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6069eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30a909a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                                              logits=logits))\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee08f92e",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant passer à la boucle d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8e49285",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2e9dd21d",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [32,300], In[1]: [64,10] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25258/1030259866.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# calcul des gradients dans le \"Gradient Tape\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25258/2786163756.py\u001b[0m in \u001b[0;36mnn_model\u001b[0;34m(x_input, W1, b1, W2, b2)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/reinf_lear/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/reinf_lear/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [32,300], In[1]: [64,10] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "total_batch = int(len(y_train) / batch_size)\n",
    "\n",
    "# boucle sur les epochs\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    avg_loss = 0\n",
    "    \n",
    "    # boucle sur les batchs\n",
    "    for i in range(total_batch):\n",
    "        \n",
    "        # extraire un minibatch des donnés\n",
    "        batch_x, batch_y = get_batch(x_train, y_train, batch_size=batch_size)\n",
    "        \n",
    "        # creations des tensors avec les données d'entrainement\n",
    "        batch_x = tf.Variable(batch_x)\n",
    "        batch_y = tf.Variable(batch_y)\n",
    "        # transformer les labels en one-hot-encoding\n",
    "        batch_y = tf.one_hot(batch_y, 10)\n",
    "        \n",
    "        # calcul des gradients dans le \"Gradient Tape\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = nn_model(batch_x, W1, b1, W2, b2)\n",
    "            loss = loss_fn(logits, batch_y)\n",
    "        gradients = tape.gradient(loss, [W1, b1, W2, b2])\n",
    "        \n",
    "        # appliquer les gradients dans l'optimiseur \"stochastic gradient descent\" pour la mise à jour des poids\n",
    "        optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))\n",
    "        \n",
    "        # mise à jour calcul loss\n",
    "        avg_loss += loss / total_batch\n",
    "    \n",
    "    # évaluation du modèle sur les données de test (validation)\n",
    "    test_logits = nn_model(x_test, W1, b1, W2, b2)\n",
    "    max_idxs = tf.argmax(test_logits, axis=1)\n",
    "    test_acc = np.sum(max_idxs.numpy() == y_test) / len(y_test)\n",
    "    print(f\"Epoch: {epoch + 1}, loss={avg_loss:.3f}, test set      accuracy={test_acc*100:.3f}%\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6b529",
   "metadata": {},
   "source": [
    "## Biblio\n",
    "\n",
    "* https://medium.com/analytics-vidhya/how-to-write-a-neural-network-in-tensorflow-from-scratch-without-using-keras-e056bb143d78\n",
    "\n",
    "* https://adventuresinmachinelearning.com/python-tensorflow-tutorial/\n",
    "\n",
    "* https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "\n",
    "* https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12817300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d36b1024f4854ed1bc7aee6b8e0d5bbafe4479b328b636d4212d10594cd9b26"
  },
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
